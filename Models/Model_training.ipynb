{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model Building and Training Notebook\n",
        "\n",
        "### This code is in developmental stage. Later will be translated for Azure Cloud implementation. \n",
        "\n",
        "### To use this code, dataset has been stored in google drive and gdrive path is mounted for use in notebook. "
      ],
      "metadata": {
        "id": "V11_XlN7pR7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import os, sys \n",
        "  #to be able to interact with Google Drive's operating system\n",
        "  from google.colab import drive \n",
        "  #drive is a module that allows us use Python to interact with google drive\n",
        "  drive.mount('/content/gdrive') \n",
        "  #mounting google drive allows us to work with its contents\n",
        "  nb_path = '/content/notebooks'\n",
        "  os.symlink('/content/gdrive/My Drive/Colab Notebooks', nb_path)\n",
        "  sys.path.insert(0, nb_path)  # or append(nb_path)\n",
        "  #The last three lines are what changes the path of the file.\n",
        "except:\n",
        "  print(\"Drive already mounted and ready to use!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baqnWATzpPWB",
        "outputId": "4d75e965-4486-42da-a516-1ca1b7f4cae0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Drive already mounted and ready to use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If there is some error message in the above cell, then no worries, just proceed running from the below cell. It's just that you must have mounted the drive already and re-running the cell for mounting your drive again."
      ],
      "metadata": {
        "id": "FZ6f73gtxiC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCNrsM5ZpQOG",
        "outputId": "7b2f5528-72f8-4b02-b3b5-fbe6e8b02c7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "rT17viZMpQRH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing all required paths for later use\n",
        "\n",
        "main_cwd = r'/content/gdrive/My Drive/Colab Notebooks'\n",
        "model_cwd = os.path.join(main_cwd, \"Models\")\n",
        "dataset_cwd = os.path.join(main_cwd, \"Datasets\")\n",
        "train_dataset_cwd = os.path.join(dataset_cwd, \"train\")\n",
        "validation_dataset_cwd = os.path.join(dataset_cwd, \"validation\")"
      ],
      "metadata": {
        "id": "Gclzrlvhlz-v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4M9KYt1Oq-8N"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "import warnings\n",
        "import re\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "bQfjT_DEtEPN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "Tp7p9tGkz-bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cwd_files(path):\n",
        "\n",
        "  ignore_files = [\".gitkeep\", \".gitignore\"]\n",
        "  path_files = os.listdir(path)\n",
        "  path_files = [file for file in path_files if file not in ignore_files]\n",
        "\n",
        "  return path_files"
      ],
      "metadata": {
        "id": "5adZGsvUuJI9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_data(image_data_process_directory, image_resize_value):\n",
        "\n",
        "  # Classes for our prediction\n",
        "  classification_classes = [\"healthy_bones\", \"fractured_bones\"]\n",
        "  classification_classes_dict = {1:\"healthy_bones\", 2:\"fractured_bones\", 3:\"bones_beyond_repair\"}\n",
        "  # Set Image size\n",
        "  img_size = 224\n",
        "\n",
        "  # Processing image to array\n",
        "  data = []\n",
        "  for each_class in classification_classes:\n",
        "    class_category_number = classification_classes.index(each_class)\n",
        "    bone_class_img_path = os.path.join(image_data_process_directory, each_class)\n",
        "    #print(bone_class_img_path)\n",
        "    bone_categories = get_cwd_files(bone_class_img_path)\n",
        "    for bone_category in bone_categories:\n",
        "      bone_category_img_path = os.path.join(bone_class_img_path, bone_category)\n",
        "      #print(bone_category_img_path)\n",
        "      patients_recorded = get_cwd_files(bone_category_img_path)\n",
        "      #print(len(patients_recorded))\n",
        "      for patient_record in patients_recorded:\n",
        "        patient_record_img_path = os.path.join(bone_category_img_path, patient_record)\n",
        "        patient_record_files = get_cwd_files(patient_record_img_path)\n",
        "        #print(patient_record_files)\n",
        "        for patient_record_file in patient_record_files:\n",
        "          patient_record_file_path = os.path.join(patient_record_img_path, patient_record_file)\n",
        "          patient_record_case_images = get_cwd_files(patient_record_file_path)\n",
        "          #print(len(patient_record_case_images))\n",
        "          for patient_image in patient_record_case_images:\n",
        "            #print(patient_image)\n",
        "            patient_image_path = os.path.join(patient_record_file_path, patient_image)\n",
        "            #print(patient_image_path)\n",
        "            try:\n",
        "              x_ray_image = load_img(patient_image_path, target_size=(image_resize_value, image_resize_value))\n",
        "              x_ray_image = img_to_array(x_ray_image)\n",
        "              #print(x_ray_image.shape)\n",
        "              data.append([x_ray_image, class_category_number])\n",
        "            except:\n",
        "              print(\"Some error occured in fetching data!\")\n",
        "\n",
        "  return np.array(data)\n"
      ],
      "metadata": {
        "id": "-UZbxD1CuEvy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Program"
      ],
      "metadata": {
        "id": "0DVsOQeG0GTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing train data images\n",
        "train_data = get_image_data(train_dataset_cwd, image_resize_value = 224)"
      ],
      "metadata": {
        "id": "2ei3jZBovePp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly_3vKY6wXXb",
        "outputId": "ba279540-8c05-4741-ab05-1abed3f5319e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5543, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bnpWAouBiYXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0zTlKDkAwjbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nZkY-4C0wjeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BsJueHnTwjgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v5ZVDwjGwjjE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}