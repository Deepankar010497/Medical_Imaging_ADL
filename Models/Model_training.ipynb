{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model Building and Training Notebook\n",
        "\n",
        "### This code is in developmental stage. Later will be translated for Azure Cloud implementation. \n",
        "\n",
        "### To use this code, dataset has been stored in google drive and gdrive path is mounted for use in notebook. "
      ],
      "metadata": {
        "id": "V11_XlN7pR7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import os, sys \n",
        "  #to be able to interact with Google Drive's operating system\n",
        "  from google.colab import drive \n",
        "  #drive is a module that allows us use Python to interact with google drive\n",
        "  drive.mount('/content/gdrive') \n",
        "  #mounting google drive allows us to work with its contents\n",
        "  nb_path = '/content/notebooks'\n",
        "  os.symlink('/content/gdrive/My Drive/Colab Notebooks', nb_path)\n",
        "  sys.path.insert(0, nb_path)  # or append(nb_path)\n",
        "  #The last three lines are what changes the path of the file.\n",
        "except:\n",
        "  print(\"Drive already mounted and ready to use!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baqnWATzpPWB",
        "outputId": "5f90d95e-e717-4da3-fd6c-cef8b414fc18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Drive already mounted and ready to use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCNrsM5ZpQOG",
        "outputId": "0eb29825-5623-4d40-f834-c68c9787ac7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "rT17viZMpQRH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing all required paths for later use\n",
        "\n",
        "main_cwd = r'/content/gdrive/My Drive/Colab Notebooks/CNN_Medical_Imaging'\n",
        "model_cwd = os.path.join(main_cwd, \"Models\")\n",
        "dataset_cwd = os.path.join(main_cwd, \"Datasets\")\n",
        "image_data_cwd = os.path.join(main_cwd, \"Saved Image Data Arrays\")\n",
        "train_dataset_cwd = os.path.join(dataset_cwd, \"train\")\n",
        "test_dataset_cwd = os.path.join(dataset_cwd, \"test\")\n",
        "validation_dataset_cwd = os.path.join(dataset_cwd, \"valid\")\n",
        "train_x_image_data = os.path.join(image_data_cwd, \"train_x_images_compressed_data_array_224.npz\")\n",
        "train_y_labels_data = os.path.join(image_data_cwd, \"train_y_labels_compressed_array_224.npy\")\n",
        "test_x_image_data = os.path.join(image_data_cwd, \"test_x_images_compressed_data_array_224.npz\")\n",
        "test_y_labels_data = os.path.join(image_data_cwd, \"test_y_labels_compressed_array_224.npy\")\n",
        "valid_x_image_data = os.path.join(image_data_cwd, \"valid_x_images_compressed_data_array_224.npz\")\n",
        "valid_y_labels_data = os.path.join(image_data_cwd, \"valid_y_labels_compressed_array_224.npy\")"
      ],
      "metadata": {
        "id": "Gclzrlvhlz-v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4M9KYt1Oq-8N"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import warnings\n",
        "import re\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "bQfjT_DEtEPN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "Tp7p9tGkz-bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cwd_files(path):\n",
        "\n",
        "  ignore_files = [\".gitkeep\", \".gitignore\"]\n",
        "  path_files = os.listdir(path)\n",
        "  path_files = [file for file in path_files if file not in ignore_files]\n",
        "\n",
        "  return path_files"
      ],
      "metadata": {
        "id": "5adZGsvUuJI9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_data(image_data_process_directory, image_resize_value):\n",
        "\n",
        "  # Classes for our prediction\n",
        "  classification_classes = [\"healthy_bones\", \"fractured_bones\"]\n",
        "  classification_classes_dict = {1:\"healthy_bones\", 2:\"fractured_bones\", 3:\"bones_beyond_repair\"}\n",
        "  # Set Image size\n",
        "  img_size = image_resize_value\n",
        "\n",
        "  flag_counter = 0\n",
        "  # Processing image to array\n",
        "  #data = []\n",
        "  image_data = []\n",
        "  label_data = []\n",
        "  for each_class in classification_classes:\n",
        "    #flag_counter = 0\n",
        "    class_category_number = classification_classes.index(each_class)\n",
        "    bone_class_img_path = os.path.join(image_data_process_directory, each_class)\n",
        "    #print(bone_class_img_path)\n",
        "    bone_categories = get_cwd_files(bone_class_img_path)\n",
        "    for bone_category in bone_categories:\n",
        "      bone_category_img_path = os.path.join(bone_class_img_path, bone_category)\n",
        "      #print(bone_category_img_path)\n",
        "      patients_recorded = get_cwd_files(bone_category_img_path)\n",
        "      #print(len(patients_recorded))\n",
        "      for patient_record in patients_recorded:\n",
        "        '''\n",
        "        flag_counter += 1\n",
        "        if flag_counter == 10:\n",
        "          break'''\n",
        "        patient_record_img_path = os.path.join(bone_category_img_path, patient_record)\n",
        "        patient_record_files = get_cwd_files(patient_record_img_path)\n",
        "        #print(patient_record_files)\n",
        "        for patient_record_file in patient_record_files:\n",
        "          patient_record_file_path = os.path.join(patient_record_img_path, patient_record_file)\n",
        "          patient_record_case_images = get_cwd_files(patient_record_file_path)\n",
        "          #print(len(patient_record_case_images))\n",
        "          for patient_image in patient_record_case_images:\n",
        "            #print(patient_image)\n",
        "            patient_image_path = os.path.join(patient_record_file_path, patient_image)\n",
        "            #print(patient_image_path)\n",
        "            try:\n",
        "              x_ray_image = load_img(patient_image_path, target_size=(img_size, img_size))\n",
        "              x_ray_image = img_to_array(x_ray_image)\n",
        "              image_data.append(x_ray_image)\n",
        "              label_data.append(class_category_number)\n",
        "              #print(x_ray_image.shape)\n",
        "              #data.append([x_ray_image, class_category_number])\n",
        "            except:\n",
        "              print(\"Some error occured in fetching data!\")\n",
        "\n",
        "  \n",
        "  #data = np.array(data)\n",
        "  image_data = np.array(image_data)\n",
        "  label_data = np.array(label_data) \n",
        "  \n",
        "  return image_data, label_data  \n"
      ],
      "metadata": {
        "id": "-UZbxD1CuEvy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Program"
      ],
      "metadata": {
        "id": "0DVsOQeG0GTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre processing images to arrays and storing them is done only for the first run and for subsequent runs load the saved array images files for further work to reduce time."
      ],
      "metadata": {
        "id": "f7_LWgk5d_wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing train data images\n",
        "#train_x, train_y = get_image_data(train_dataset_cwd, image_resize_value = 224)"
      ],
      "metadata": {
        "id": "2ei3jZBovePp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing test data images\n",
        "#test_x, test_y = get_image_data(test_dataset_cwd, image_resize_value = 224)"
      ],
      "metadata": {
        "id": "mmvHIU9_cMF4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing validation data images\n",
        "#valid_x, valid_y = get_image_data(validation_dataset_cwd, image_resize_value = 224)"
      ],
      "metadata": {
        "id": "grVAQHAQcMJA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing values of the image processed array data in csv files"
      ],
      "metadata": {
        "id": "PmETMCmoQ0yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing Train image array values\n",
        "\n",
        "# For train_x compressed values\n",
        "#np.savez_compressed(\"train_x_images_compressed_data_array_224\", train_array = train_x)\n",
        "\n",
        "# For train_y compressed values\n",
        "#np.save(\"train_y_labels_compressed_array_224\", train_y)\n"
      ],
      "metadata": {
        "id": "iPOnKQyDQ02H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing Test image array values\n",
        "\n",
        "# For test_x compressed values\n",
        "#np.savez_compressed(\"test_x_images_compressed_data_array_224\", test_array = test_x)\n",
        "\n",
        "# For test_y compressed values\n",
        "#np.save(\"test_y_labels_compressed_array_224\", test_y)\n"
      ],
      "metadata": {
        "id": "ZpuTO9OTYGll"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing Validation image array values\n",
        "\n",
        "# For valid_x compressed values\n",
        "#np.savez_compressed(\"valid_x_images_compressed_data_array_224\", valid_array = valid_x)\n",
        "\n",
        "# For valid_y compressed values\n",
        "#np.save(\"valid_y_labels_compressed_array_224\", valid_y)\n"
      ],
      "metadata": {
        "id": "mw3dY6m5YGuk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Image arrays from the saved files"
      ],
      "metadata": {
        "id": "A799XQnuY12e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Train image compressed array values\n",
        "\n",
        "# For train_x values\n",
        "model_train_x = np.load(train_x_image_data)[\"train_array\"]\n",
        "\n",
        "# For train_y values\n",
        "model_train_y = np.load(train_y_labels_data)"
      ],
      "metadata": {
        "id": "PaMVEi5SY2kH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Test image compressed array values\n",
        "\n",
        "# For test_x values\n",
        "model_test_x = np.load(test_x_image_data)[\"test_array\"]\n",
        "\n",
        "# For test_y values\n",
        "model_test_y = np.load(test_y_labels_data)\n"
      ],
      "metadata": {
        "id": "s5t8ZOyXgk-L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Valid image compressed array values\n",
        "\n",
        "# For valid_x values\n",
        "model_valid_x = np.load(valid_x_image_data)[\"valid_array\"]\n",
        "\n",
        "# For valid_y values\n",
        "model_valid_y = np.load(valid_y_labels_data)\n"
      ],
      "metadata": {
        "id": "7XVW3MeDnVCF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing values\n",
        "model_train_x, model_train_y = model_train_x/255, model_train_y\n",
        "model_test_x, model_test_y = model_test_x/255, model_test_y\n",
        "model_valid_x, model_valid_y = model_valid_x/255, model_valid_y"
      ],
      "metadata": {
        "id": "HragKFD5nIPP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters for model building"
      ],
      "metadata": {
        "id": "DrSC_r1V9Hje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for Imagedatagenerator\n",
        "model_shear_range = 0.2           # Image angular deformation range\n",
        "model_zoom_range = 0.2            # Image zoom in/out range\n",
        "model_brightness_range = [0.9,1.2]    # Brightness change range (0.9 to 1.2)\n",
        "model_rotation_range = 30         # Angular change range\n",
        "model_width_shift_range = 0.2     # Horizontal shift range\n",
        "model_height_shift_range = 0.2    # Vertical shift range\n",
        "\n",
        "# Model hyperparameters\n",
        "\n",
        "model_accuracy = [\"acc\", \"mse\"]\n",
        "model_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model_lr = 0.001  # 0.000001\n",
        "model_optimizer = Adam(learning_rate = model_lr)\n",
        "model_epochs = 200\n",
        "model_callbacks = [\n",
        "                # ModelCheckpoint(checkpoint_dir, monitor = \"val_loss\", save_best_only=True, mode=\"min\",options=None),\n",
        "                # EarlyStopping(monitor=\"val_loss\",min_delta=1e-4,patience=8,verbose=1, mode=\"min\",restore_best_weights=True),\n",
        "                # ReduceLROnPlateau(monitor=\"val_loss\",min_delta=1e-4, factor=0.1, patience=3, verbose=1, min_lr=0.0001,mode=\"min\")                \n",
        "                ]\n"
      ],
      "metadata": {
        "id": "IFO98gPvD9Po"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation Initilization\n"
      ],
      "metadata": {
        "id": "rXf0yjQ7D9SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image generator for training images\n",
        "train_gen = ImageDataGenerator(\n",
        "    rotation_range = model_rotation_range,\n",
        "    brightness_range = model_brightness_range,\n",
        "    width_shift_range = model_width_shift_range,\n",
        "    height_shift_range = model_height_shift_range,\n",
        "    zoom_range = model_zoom_range,\n",
        "    shear_range = model_shear_range,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode = \"constant\",\n",
        "    cval = 0.0  # fill with black color for any blank spaces present after image shift\n",
        ")\n",
        "\n",
        "# Image generator fo validation images\n",
        "valid_gen = ImageDataGenerator()  # No parameters"
      ],
      "metadata": {
        "id": "iqa5a3T29HmN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation"
      ],
      "metadata": {
        "id": "CmwF4iNNzDs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting image generator models to datasets \n",
        "train_gen.fit(model_train_x)\n",
        "valid_gen.fit(model_valid_x)\n",
        "\n",
        "# Creating iter variables for model compile flow\n",
        "train_iter = train_gen.flow(model_train_x, model_train_y, batch_size = 32)\n",
        "valid_iter = valid_gen.flow(model_valid_x, model_valid_y, batch_size = 8, shuffle = False)"
      ],
      "metadata": {
        "id": "_GtxqZrxzDv7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eyxyNfDocMng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tTcWRNQNCDHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3Hzrpl8xCDKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pXN3lrADCDNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eA8dKspqcMqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}